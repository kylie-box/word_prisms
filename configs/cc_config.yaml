embedding:
  exp_embs:
    - win10
    - fasttext2
  emb_dropout: 0.5
  embeds_root: /home/jingyihe/projects/def-dprecup/jingyihe/files/embeddings
  normalize: false
  zero_mean: True
  truncation: 20
  projection: true
  proj_dim: 256
  nonlinear: true
  projection_normalization: False # normalize projected embeddings

evaluation:
  dataset_dir: /home/jingyihe/scratch/wp/evaluation/
  prism_level: system   # choices: system, word, contextualized
  average_baseline: false
  concat_baseline: true
  parameterization_type: None
  proj_normalization: false
  proj_lambda: 0
  orthogonal: false
  beta: 0.001
  clf_dropout: 0.5
  sentence_repr_lc: false   # ignore
  max_pooling: true
  rnn_dim: 512
  rnn_dropout: 0.5
  fc_dim: 512
  task: semcor-sst
  hilbert: false
  lstm_layers: 1
  monitor_word: cat
  lc_weights_loss: None
  cdme: false

checkpoint: None
epoch: 100
grad_clip: 5.0
lr: 4.0e-4
lr_min: 8.0e-5
lr_shrink: 0.2
max_epochs: 100
mb_size: 64   # 64 for classification problems and 16 for seqlab should be fine

optimizer: adam
output_dir: /home/jingyihe/scratch/wp/experiments
seed: 619
early_stop_patience: 10
write_logger: false
verbose: true
cached: false
checkpoint_frequency: 2
